{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDrx1eZwQeSl"
      },
      "outputs": [],
      "source": [
        "input_path = 'result_remaining.csv'\n",
        "output_path = 'cleaned__remaining_results.csv'\n",
        "\n",
        "with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8') as outfile:\n",
        "    for line in infile:\n",
        "        line = line.rstrip('\\n')  # remove trailing newline\n",
        "\n",
        "        # Check if the line ends with one of the patterns and replace\n",
        "        for i in range(6):  # 0 to 4\n",
        "            if line.endswith(f'. Score: {i}'):\n",
        "                line = line[:-len(f'. Score: {i}')] + f', Score:{i}'\n",
        "                break  # only one match per line\n",
        "\n",
        "        outfile.write(line + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = 'cleaned__remaining_results.csv'\n",
        "output_path = 'cleaned__remaining_results_1.csv'\n",
        "\n",
        "with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8') as outfile:\n",
        "    for line in infile:\n",
        "        line = line.rstrip('\\n')  # remove trailing newline\n",
        "\n",
        "        # Check if the line ends with one of the patterns and replace\n",
        "        for i in range(6):  # 0 to 4\n",
        "            if line.endswith(f'. Score:{i}'):\n",
        "                line = line[:-len(f'. Score:{i}')] + f', Score:{i}'\n",
        "                break  # only one match per line\n",
        "\n",
        "        outfile.write(line + '\\n')"
      ],
      "metadata": {
        "id": "zYIfEwPwQlpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#double space after \". Score:i  \"\n",
        "input_path = 'cleaned__remaining_results_1.csv'\n",
        "output_path = 'cleaned__remaining_results_2.csv'\n",
        "\n",
        "with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8') as outfile:\n",
        "    for line in infile:\n",
        "        line = line.rstrip('\\n')  # remove trailing newline\n",
        "\n",
        "        # Check if the line ends with one of the patterns and replace\n",
        "        for i in range(6):  # 0 to 4\n",
        "            if line.endswith(f'. Score:{i}  '):\n",
        "                line = line[:-len(f'. Score:{i}')] + f', Score:{i}'\n",
        "                break  # only one match per line\n",
        "\n",
        "        outfile.write(line + '\\n')"
      ],
      "metadata": {
        "id": "02Ond6n_QojA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "input_path = 'cleaned__remaining_results_2.csv'\n",
        "output_path = 'cleaned__remaining_results_3.csv'\n",
        "\n",
        "with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8') as outfile:\n",
        "    for line in infile:\n",
        "        line = line.rstrip('\\n')  # remove trailing newline\n",
        "\n",
        "        # Check if the line ends with one of the patterns and replace\n",
        "        for i in range(6):  # 0 to 4\n",
        "            if line.endswith(f'. Score: {i}  '):\n",
        "                line = line[:-len(f'. Score:{i}')] + f', Score:{i}'\n",
        "                break  # only one match per line\n",
        "\n",
        "        outfile.write(line + '\\n')"
      ],
      "metadata": {
        "id": "setl48ZTQq6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\". Score: i.\"\n",
        "input_path = 'cleaned__remaining_results_3.csv'\n",
        "output_path = 'cleaned__remaining_results_4.csv'\n",
        "\n",
        "with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8') as outfile:\n",
        "    for line in infile:\n",
        "        line = line.rstrip('\\n')  # remove trailing newline\n",
        "\n",
        "        # Check if the line ends with one of the patterns and replace\n",
        "        for i in range(6):  # 0 to 4\n",
        "            if line.endswith(f'. Score: {i}.'):\n",
        "                line = line[:-len(f'. Score:{i}')] + f', Score:{i}'\n",
        "                break  # only one match per line\n",
        "\n",
        "        outfile.write(line + '\\n')"
      ],
      "metadata": {
        "id": "OyifNtflQyCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\n",
        "    \"EducationalPython\",\n",
        "    \"ArithmeticOperation\",\n",
        "    \"CheckingCondition\",\n",
        "    \"CheckingInForLoop\",\n",
        "    \"CheckingInWhileLoop\",\n",
        "    \"DataTypeSpecificTask\",\n",
        "    \"HandlingDictionary\",\n",
        "    \"HandlingFile\",\n",
        "    \"HandlingFloat\",\n",
        "    \"HandlingInteger\",\n",
        "    \"HandlingList\",\n",
        "    \"AddingListElement\",\n",
        "    \"HandlingString\",\n",
        "    \"ModifyingStringCase\",\n",
        "    \"HandlingTuple\",\n",
        "    \"HandlingAdvancedDataStructure\",\n",
        "    \"TwoDimensionalArrayTask\",\n",
        "    \"HandlingError\",\n",
        "    \"HandlingFunction\",\n",
        "    \"HandlingFunctionCallingFunction\",\n",
        "    \"HandlingNestedFunction\",\n",
        "    \"HandlingRecursiveFunction\",\n",
        "    \"HandlingStandardFunction\",\n",
        "    \"HandlingObjectOrientedProgramming\",\n",
        "    \"Iteration\",\n",
        "    \"ForLoopWithUpdate\",\n",
        "    \"NestedLoopIteration\",\n",
        "    \"WhileLoopWithUpdate\",\n",
        "    \"ManagingVariable\",\n",
        "    \"Python\",\n",
        "    \"BuiltInFunction\",\n",
        "    \"Exception\",\n",
        "    \"Error\",\n",
        "    \"Expression\",\n",
        "    \"ArithmeticExpression\",\n",
        "    \"Call\",\n",
        "    \"Literal\",\n",
        "    \"LiteralCollection\",\n",
        "    \"NumericLiteral\",\n",
        "    \"LogicalExpression\",\n",
        "    \"SubscriptExpression\",\n",
        "    \"Method\",\n",
        "    \"ClassMethod\",\n",
        "    \"DictionaryMethod\",\n",
        "    \"FileMethod\",\n",
        "    \"ListMethod\",\n",
        "    \"StringMethod\",\n",
        "    \"Operator\",\n",
        "    \"ArtithmeticOperator\",\n",
        "    \"ComparisonOperator\",\n",
        "    \"LogicalOperator\",\n",
        "    \"MembershipOperator\",\n",
        "    \"Statement\",\n",
        "    \"AssignmentStatement\",\n",
        "    \"ConditionalStatement\",\n",
        "    \"DefinitionStatement\",\n",
        "    \"ExceptionHandlingStatement\",\n",
        "    \"ImportStatement\",\n",
        "    \"IterationStatement\",\n",
        "    \"JumpStatement\"\n",
        "]\n",
        "\n",
        "leaf_class= [\n",
        "    \"Addition\", \"Division\", \"FindAbsolute\", \"FindMaximum\", \"FindMinimum\", \"FindRemainder\", \"Multiplication\",\n",
        "    \"Power\", \"Round\", \"Substraction\", \"AssertChecking\", \"BooleanEvaluating\", \"IfCheckingInForLoop\",\n",
        "    \"IfElifCheckingInForLoop\", \"IfElifElseCheckingInForLoop\", \"IfElseCheckingInForLoop\", \"IfCheckingInWhileLoop\",\n",
        "    \"IfElifCheckingInWhileLoop\", \"IfElifElseCheckingInWhileLoop\", \"IfElseCheckingInWhileLoop\", \"IfChecking\",\n",
        "    \"IfElifChecking\", \"IfElifElseChecking\", \"IfElseChecking\", \"NestedIfChecking\", \"CheckingForItem\", \"Concatenation\",\n",
        "    \"AccessingDictionary\", \"AddingItemToDictionary\", \"CreatingDictionary\", \"IndexingDictionary\",\n",
        "    \"ReplacingDictionaryElement\", \"ClosingFile\", \"OpeningFile\", \"ReadingLine\", \"TransformingToFloatType\",\n",
        "    \"TransformingToIntType\", \"AddingListElementWithAppend\", \"AddingListElementWithInsert\", \"CreatingList\",\n",
        "    \"DeletingListElement\", \"IndexingList\", \"ListReferencing\", \"ReplacingListElement\", \"ReversingList\", \"SlicingList\",\n",
        "    \"CreatingString\", \"FormattingString\", \"IndexingString\", \"CapitalizeString\", \"ConvertStringToLowerCase\",\n",
        "    \"ConvertStringToUpperCase\", \"ReplacingStringContent\", \"SlicingString\", \"SplittingString\",\n",
        "    \"TransformingToStringType\", \"TrimmingString\", \"CreatingTuple\", \"IndexingTuple\", \"SlicingTuple\", \"FindingLength\",\n",
        "    \"BinaryTreeTask\", \"LinkedListTask\", \"QueueTask\", \"StackTask\", \"Create2DArray\", \"Indexing2DArray\",\n",
        "    \"ReplacingElement2DArray\", \"HandlingGeneralError\", \"HandlingSpecificError\", \"CallingFunctionLibrary\",\n",
        "    \"CallingFunctionCallingFunction\", \"DefiningFunctionCallingFunction\", \"CallingNestedFunction\",\n",
        "    \"DefiningNestedFunction\", \"CallingRecursiveFunction\", \"DefiningRecursiveFunction\", \"CallingStandardFunction\",\n",
        "    \"DefiningStandardFunction\", \"NestedFunctionCall\", \"HandlingInput\", \"CallingOOPMethod\", \"CreatingObject\",\n",
        "    \"DefiningClass\", \"DefiningOOPMethod\", \"Importing\", \"ForLoopWithListIndexing\", \"ForLoopWith*=\",\n",
        "    \"ForLoopWith+=\", \"MixedNestedLoopIteration\", \"NestedForLoopIteration\", \"NestedWhileLoopIteration\",\n",
        "    \"SingleForLoopIteration\", \"SingleWhileLoopIteration\", \"WhileLoopWithListIndexing\", \"WhileLoopWith*=\",\n",
        "    \"WhileLoopWith+=\", \"AssigningVariable\", \"SwappingVariablesValue\", \"UpdatingVariable\", \"Printing\", \"abs()\", \"dict()\", \"float()\", \"input()\", \"int()\", \"len()\", \"list()\", \"max()\", \"min()\",\n",
        "    \"next()\", \"open()\", \"print()\", \"range()\", \"round()\", \"set()\", \"str()\", \"tuple()\",\n",
        "    \"AssertionError\", \"FileNotFoundError\", \"IOError\", \"IndexError\", \"OSError\", \"TypeError\", \"ValueError\",\n",
        "    \"BinaryOperation\", \"UnaryOperation\", \"FunctionCall\", \"BooleanLiteral\", \"DictLiteral\", \"ListLiteral\",\n",
        "    \"SetLiteral\", \"TupleLiteral\", \"NoneLiteral\", \"FloatLiteral\", \"IntegerLiteral\", \"StringLiteral\",\n",
        "    \"BooleanExpression\", \"ComparisonExpression\", \"IndexingExpression\", \"SlicingExpression\",\n",
        "    \"__init()__\", \"get()\", \"items()\", \"keys()\", \"values()\", \"close()\", \"read()\", \"readline()\",\n",
        "    \"append()\", \"insert()\", \"pop()\", \"remove()\", \"reverse()\", \"capitalize()\", \"format()\", \"join()\",\n",
        "    \"lower()\", \"replace()\", \"rstrip()\", \"split()\",\n",
        "    \"Modulus\", \"*\", \"**\", \"+\", \"-\", \"/\", \"//\", \"Larger\", \"LargerOrEqual\", \"Less\", \"LessOrEqual\",\n",
        "    \"NotEqual\", \"==\", \"and\", \"not\", \"or\", \"in\", \"=\",\n",
        "    \"AugmentedAssignmentStatement\", \"SimpleAssignmentStatement\", \"IfElifElseStatement\",\n",
        "    \"IfElifStatement\", \"IfElseStatement\", \"IfStatement\", \"ClassDefinition\", \"FunctionDefinition\",\n",
        "    \"AssertStatement\", \"RaiseStatement\", \"TryStatement\", \"ExpressionStatement\", \"Import\",\n",
        "    \"ImportFrom\", \"ForStatement\", \"WhileStatement\", \"BreakStatement\", \"ContinueStatement\",\n",
        "    \"PassStatement\", \"ReturnStatement\"\n",
        "]\n",
        "\n",
        "# File paths (you can change these)\n",
        "input_file_path = 'cleaned__remaining_results_4.csv'\n",
        "output_file_path = 'cleaned__remaining_results_5.csv'\n",
        "# Process the file\n",
        "with open(input_file_path, \"r\", encoding=\"utf-8\") as infile, open(output_file_path, \"w\", encoding=\"utf-8\") as outfile:\n",
        "    for line in infile:\n",
        "        # Check if there's a double comma\n",
        "        if \",,\" in line:\n",
        "            parts = line.strip().split(\",\")\n",
        "            # Check if the second entry is in 'classes' and the third entry is either empty or in 'leaf_class'\n",
        "            if len(parts) >= 4 and parts[2] in classes and (parts[3] == \"\" or parts[3] in leaf_class):\n",
        "                # Remove the empty second field (comma) and fix the line\n",
        "                parts = [parts[0]] + parts[2:]\n",
        "                line = \",\".join(parts) + \"\\n\"\n",
        "        outfile.write(line)\n",
        "\n"
      ],
      "metadata": {
        "id": "x6qDft8oQ0TA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file_path = 'cleaned__remaining_results_5.csv'\n",
        "output_file_path = 'cleaned__remaining_results_6.csv'\n",
        "\n",
        "with open(input_file_path, \"r\", encoding=\"utf-8\") as infile, open(output_file_path, \"w\", encoding=\"utf-8\") as outfile:\n",
        "    for line in infile:\n",
        "        parts = line.strip().split(\",\")\n",
        "        if len(parts) >= 4:\n",
        "            third = parts[2]\n",
        "            fourth = parts[3].strip()\n",
        "\n",
        "            # Check the conditions\n",
        "            if (\n",
        "                third in leaf_class and\n",
        "                (fourth in {\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"} or fourth.lower().startswith(\"score\"))\n",
        "            ):\n",
        "                # Insert an empty string before the 4th element\n",
        "                parts = parts[:3] + [\"\"] + parts[3:]\n",
        "                line = \",\".join(parts) + \"\\n\"\n",
        "\n",
        "        outfile.write(line)"
      ],
      "metadata": {
        "id": "InG_0QYLQ29x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('cleaned__remaining_results_6.csv', \"r\") as infile, open('cleaned__remaining_results_7.csv', \"w\") as outfile:\n",
        "    for line in infile:\n",
        "        parts = line.strip().split(',')\n",
        "        if len(parts) >= 2 and '.' in parts[1]:\n",
        "            prefix = parts[1].split('.', 1)[0]\n",
        "            if prefix in classes:\n",
        "                parts[1] = parts[1].replace('.', ',', 1)\n",
        "        outfile.write(','.join(parts) + '\\n')"
      ],
      "metadata": {
        "id": "BPYeO8P4Q4_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('cleaned__remaining_results_7.csv', \"r\") as infile, open('cleaned__remaining_results_8.csv', \"w\") as outfile:\n",
        "    for line in infile:\n",
        "        parts = line.strip().split(',')\n",
        "        if len(parts) >= 2 and '_' in parts[1]:\n",
        "            prefix = parts[1].split('_', 1)[0]\n",
        "            if prefix in classes:\n",
        "                parts[1] = parts[1].replace('_', ',', 1)\n",
        "        outfile.write(','.join(parts) + '\\n')"
      ],
      "metadata": {
        "id": "Dvc-HcoGQ7u-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read CSV as plain text\n",
        "with open('cleaned__remaining_results_8.csv', 'r', encoding='utf-8') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Process lines\n",
        "new_lines = []\n",
        "for line in lines:\n",
        "    parts = line.strip().split(',')\n",
        "    if parts[0] == 'q_py_2d_arrays1':\n",
        "        parts[0] = 'q_py_topic_two_dimensional_array:q_py_2d_arrays1'\n",
        "    elif parts[0] == 'q_py_2d_arrays2':\n",
        "        parts[0] = 'q_py_topic_two_dimensional_array:q_py_2d_arrays2'\n",
        "    elif parts[0] == 'q_py_account_logic1':\n",
        "        parts[0] = 'q_py_topic_logical_operators:q_py_account_logic1'\n",
        "    elif parts[0] == 'q_py_dict_access1':\n",
        "        parts[0] = 'q_py_topic_dictionary:q_py_dict_access1'\n",
        "    elif parts[0] == 'q_py_exceptions_addarr':\n",
        "        parts[0] = 'q_py_topic_exceptions:q_py_exceptions_addarr'\n",
        "    elif parts[0] == 'q_py_exchange1':\n",
        "        parts[0] = 'q_py_topic_variables:q_py_exchange1'\n",
        "    elif parts[0] == 'q_py_fun_car1':\n",
        "        parts[0] = 'q_py_topic_functions:q_py_fun_car1'\n",
        "    elif parts[0] == 'q_py_fun_car2':\n",
        "        parts[0] = 'q_py_topic_functions:q_py_fun_car2'\n",
        "    elif parts[0] == 'q_py_list_except1':\n",
        "        parts[0] = 'q_py_topic_exceptions:q_py_list_except1'\n",
        "    elif parts[0] == 'q_py_topic_file_processing':\n",
        "        parts[0] = 'q_py_topic_file_processing:q_py_file1'\n",
        "    elif parts[0] == 'q_py_exceptions_addarr':\n",
        "        parts[0] = 'q_py_topic_exceptions:q_py_exceptions_addarr'\n",
        "    elif parts[0] == 'q_py_exceptions_addarr':\n",
        "        parts[0] = 'q_py_topic_exceptions:q_py_exceptions_addarr'\n",
        "    #elif parts[0] == 'py_objects_classes_tv':\n",
        "    #    parts[0] = 'objects_classes_tv'\n",
        "    #elif parts[0] == 'py_objects_classes_point':\n",
        "    #    parts[0] = 'objects_classes_point'\n",
        "    #elif parts[0] == 'py_objects_classes_loan':\n",
        "    #    parts[0] = 'objects_classes_loan'\n",
        "    #elif parts[0] == 'objects_classes_account':\n",
        "    #    parts[0] = 'py_objects_classes_account'\n",
        "    elif parts[0] == 'q_py_topic_output_formatting:q_py_output_formatting:q_py_output3':\n",
        "        parts[0] = 'q_py_topic_output_formatting:q_py_output3'\n",
        "\n",
        "\n",
        "\n",
        "    new_lines.append(','.join(parts) + '\\n')\n",
        "\n",
        "# Write the modified content back\n",
        "with open('cleaned__remaining_results_10.csv', 'w', encoding='utf-8') as file:\n",
        "    file.writelines(new_lines)\n"
      ],
      "metadata": {
        "id": "dCIXKSunRBJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_endings = {\"0\",\"1\", \"2\", \"3\", \"4\", \"5\", \" 0\", \" 1\",\" 2\",\" 3\",\" 4\",\" 5\", \" Score: 0\", \" Score: 1\", \" Score: 2\", \" Score: 3\", \" Score: 4\", \" Score: 5\", \" Score:0\", \" Score:1\", \" Score:2\", \" Score:3\", \" Score:4\", \" Score:5\", \"score=0\", \"score=1\", \"score=2\", \"score=3\", \"score=4\", \"score=5\", \"Score:0\", \"Score:1\", \"Score:2\", \"Score:3\", \"Score:4\", \"Score:5\", \" Score=0\", \" Score=1\", \" Score=2\", \" Score=3\", \" Score=4\", \" Score=5\", \" score=0\", \" score=1\", \" score=2\", \" score=3\", \" score=4\", \" score=5\", \"score:0\", \"score:1\", \"score:2\", \"score:3\", \"score:4\", \"score:5\", \"Score: 0\", \"Score: 1\", \"Score: 2\", \"Score: 3\", \"Score: 4\", \"Score: 5\", \" score is 0\", \" score is 1\", \" score is 2\", \" score is 3\", \" score is 4\", \" score is 5\", \"score is 0\", \"score is 1\", \"score is 2\", \"score is 3\", \"score is 4\", \"score is 5\",\"score 0\",\"score 1\",\"score 2\",\"score 3\",\"score 4\",\"score 5\",\" score 0\",\" score 1\",\" score 2\",\" score 3\",\" score 4\",\" score 5\", \" score: 4\",\" score: 0\",\" score: 1\",\" score: 2\",\" score: 3\",\" score: 5\",\" score:0\",\" score:1\",\" score:2\",\" score:3\",\" score:4\",\" score:5\"}\n",
        "\n",
        "with open(\"cleaned__remaining_results_10.csv\", \"r\") as infile:\n",
        "    for line in infile:\n",
        "        parts = line.strip().split(',')\n",
        "        if not parts:\n",
        "            print('empty line')\n",
        "            continue  # Skip empty lines\n",
        "        if parts[-1] not in valid_endings:\n",
        "            print(line.strip())"
      ],
      "metadata": {
        "id": "JvR6FQIHRMAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"cleaned__remaining_results_10.csv\", 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        # Strip newline and split by comma\n",
        "        parts = line.strip().split(',')\n",
        "        if parts and parts[0] == 'ps':\n",
        "            print(line.strip())  #"
      ],
      "metadata": {
        "id": "MW2xMgvqRNkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data_out='cleaned__remaining_results_10.csv'\n",
        "original_result = pd.read_csv(data_out)\n",
        "#df = pd.read_csv(data_out,header=None, on_bad_lines='skip', engine='python')\n",
        "original_result"
      ],
      "metadata": {
        "id": "juDGdy3DRPh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "note that before running the following box, columns in the csv file need to be added. Specifically from e1 to e14."
      ],
      "metadata": {
        "id": "BVBFFT3tRUPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# List of columns to check\n",
        "columns_to_check = ['e1','e2','e3','e4','e5','e6','e7','e8','e9','e10','e11','e12','e13','e14']\n",
        "\n",
        "#FIRST ITERATION\n",
        "filtered_data = original_result[original_result[columns_to_check].isna().all(axis=1)]\n",
        "filtered_data= filtered_data.drop(columns=['e1','e2','e3','e4','e5','e6','e7','e8','e9','e10','e11','e12','e13','e14', 'reason'])\n",
        "remaining_data = original_result.drop(filtered_data.index)\n",
        "\n",
        "print('first iteration : ', len(filtered_data))\n",
        "print('total:', len(filtered_data)+len(remaining_data))\n",
        "\n",
        "#SECOND ITERATION\n",
        "filtered_data_2 = remaining_data[\n",
        "    remaining_data['e1'].notna() &\n",
        "    remaining_data[columns_to_check[1:]].isna().all(axis=1)\n",
        "]\n",
        "mask = (\n",
        "    remaining_data['e1'].notna() &\n",
        "    remaining_data[columns_to_check[1:]].isna().all(axis=1)\n",
        ")\n",
        "\n",
        "filtered_data_2.loc[mask, 'score'] = filtered_data_2.loc[mask, 'e1']\n",
        "\n",
        "filtered_data_2= filtered_data_2.drop(columns=['e1','e2','e3','e4','e5','e6','e7','e8','e9','e10','e11','e12','e13','e14', 'reason'])\n",
        "final_filtered = pd.concat([filtered_data, filtered_data_2], ignore_index=True)\n",
        "remaining_data = remaining_data.drop(filtered_data_2.index)\n",
        "\n",
        "print('second  iteration : ', len(final_filtered))\n",
        "print('total:', len(final_filtered)+len(remaining_data))\n",
        "\n",
        "#THIRD  ITERATION\n",
        "filtered_data_3 = remaining_data[\n",
        "    remaining_data['e2'].notna() &\n",
        "    remaining_data[columns_to_check[2:]].isna().all(axis=1)\n",
        "]\n",
        "mask = (\n",
        "    remaining_data['e2'].notna() &\n",
        "    remaining_data[columns_to_check[2:]].isna().all(axis=1)\n",
        ")\n",
        "\n",
        "filtered_data_3.loc[mask, 'score'] = filtered_data_3.loc[mask, 'e2']\n",
        "\n",
        "filtered_data_3= filtered_data_3.drop(columns=['e1','e2','e3','e4','e5','e6','e7','e8','e9','e10','e11','e12','e13','e14', 'reason'])\n",
        "final_filtered = pd.concat([final_filtered, filtered_data_3], ignore_index=True)\n",
        "remaining_data = remaining_data.drop(filtered_data_3.index)\n",
        "\n",
        "print('third iteration : ', len(final_filtered))\n",
        "print('total:', len(final_filtered)+len(remaining_data))\n",
        "\n",
        "#FOURTH  ITERATION\n",
        "filtered_data_4 = remaining_data[\n",
        "    remaining_data['e3'].notna() &\n",
        "    remaining_data[columns_to_check[3:]].isna().all(axis=1)\n",
        "]\n",
        "\n",
        "mask = (\n",
        "    remaining_data['e3'].notna() &\n",
        "    remaining_data[columns_to_check[3:]].isna().all(axis=1)\n",
        ")\n",
        "\n",
        "filtered_data_4.loc[mask, 'score'] = filtered_data_4.loc[mask, 'e3']\n",
        "\n",
        "filtered_data_4= filtered_data_4.drop(columns=['e1','e2','e3','e4','e5','e6','e7','e8','e9','e10','e11','e12','e13','e14', 'reason'])\n",
        "\n",
        "final_filtered = pd.concat([final_filtered, filtered_data_4], ignore_index=True)\n",
        "remaining_data = remaining_data.drop(filtered_data_4.index)\n",
        "\n",
        "print('fourth iteration : ', len(final_filtered))\n",
        "print('total:', len(final_filtered)+len(remaining_data))\n",
        "\n",
        "#FIFTH ITERATION\n",
        "filtered_data_5 = remaining_data[\n",
        "    remaining_data['e4'].notna() &\n",
        "    remaining_data[columns_to_check[4:]].isna().all(axis=1)\n",
        "]\n",
        "\n",
        "mask = (\n",
        "    remaining_data['e4'].notna() &\n",
        "    remaining_data[columns_to_check[4:]].isna().all(axis=1)\n",
        ")\n",
        "\n",
        "filtered_data_5.loc[mask, 'score'] = filtered_data_5.loc[mask, 'e4']\n",
        "\n",
        "filtered_data_5= filtered_data_5.drop(columns=['e1','e2','e3','e4','e5','e6','e7','e8','e9','e10','e11','e12','e13','e14', 'reason'])\n",
        "\n",
        "final_filtered = pd.concat([final_filtered, filtered_data_5], ignore_index=True)\n",
        "remaining_data = remaining_data.drop(filtered_data_5.index)\n",
        "\n",
        "print('fifth iteration : ', len(final_filtered))\n",
        "print('total:', len(final_filtered)+len(remaining_data))\n",
        "\n",
        "\n",
        "#SIXTH ITERATION\n",
        "filtered_data_6 = remaining_data[\n",
        "    remaining_data['e5'].notna() &\n",
        "    remaining_data[columns_to_check[5:]].isna().all(axis=1)\n",
        "]\n",
        "\n",
        "mask = (\n",
        "    remaining_data['e5'].notna() &\n",
        "    remaining_data[columns_to_check[5:]].isna().all(axis=1)\n",
        ")\n",
        "\n",
        "filtered_data_6.loc[mask, 'score'] = filtered_data_6.loc[mask, 'e5']\n",
        "\n",
        "filtered_data_6= filtered_data_6.drop(columns=['e1','e2','e3','e4','e5','e6','e7','e8','e9','e10','e11','e12','e13','e14', 'reason'])\n",
        "\n",
        "final_filtered = pd.concat([final_filtered, filtered_data_6], ignore_index=True)\n",
        "remaining_data = remaining_data.drop(filtered_data_6.index)\n",
        "\n",
        "print('sixth iteration : ', len(final_filtered))\n",
        "print('total:', len(final_filtered)+len(remaining_data))\n",
        "\n",
        "#SEVENTH ITERATION\n",
        "filtered_data_7 = remaining_data[\n",
        "    remaining_data['e6'].notna() &\n",
        "    remaining_data[columns_to_check[6:]].isna().all(axis=1)\n",
        "]\n",
        "\n",
        "mask = (\n",
        "    remaining_data['e6'].notna() &\n",
        "    remaining_data[columns_to_check[6:]].isna().all(axis=1)\n",
        ")\n",
        "\n",
        "filtered_data_7.loc[mask, 'score'] = filtered_data_7.loc[mask, 'e6']\n",
        "\n",
        "filtered_data_7= filtered_data_7.drop(columns=['e1','e2','e3','e4','e5','e6','e7','e8','e9','e10','e11','e12','e13','e14', 'reason'])\n",
        "\n",
        "final_filtered = pd.concat([final_filtered, filtered_data_7], ignore_index=True)\n",
        "remaining_data = remaining_data.drop(filtered_data_7.index)\n",
        "\n",
        "print('seventh iteration : ', len(final_filtered))\n",
        "print('total:', len(final_filtered)+len(remaining_data))\n",
        "\n",
        "#EIGHTH ITERATION\n",
        "filtered_data_8 = remaining_data[\n",
        "    remaining_data['e7'].notna() &\n",
        "    remaining_data[columns_to_check[7:]].isna().all(axis=1)\n",
        "]\n",
        "\n",
        "mask = (\n",
        "    remaining_data['e7'].notna() &\n",
        "    remaining_data[columns_to_check[7:]].isna().all(axis=1)\n",
        ")\n",
        "\n",
        "filtered_data_8.loc[mask, 'score'] = filtered_data_8.loc[mask, 'e7']\n",
        "\n",
        "filtered_data_8= filtered_data_8.drop(columns=['e1','e2','e3','e4','e5','e6','e7','e8','e9','e10','e11','e12','e13','e14', 'reason'])\n",
        "\n",
        "final_filtered = pd.concat([final_filtered, filtered_data_8], ignore_index=True)\n",
        "remaining_data = remaining_data.drop(filtered_data_8.index)\n",
        "\n",
        "print('eighth iteration : ', len(final_filtered))\n",
        "print('total:', len(final_filtered)+len(remaining_data))\n",
        "\n",
        "\n",
        "#NINETH ITERATION\n",
        "filtered_data_9 = remaining_data[\n",
        "    remaining_data['e8'].notna() &\n",
        "    remaining_data[columns_to_check[8:]].isna().all(axis=1)\n",
        "]\n",
        "\n",
        "mask = (\n",
        "    remaining_data['e8'].notna() &\n",
        "    remaining_data[columns_to_check[8:]].isna().all(axis=1)\n",
        ")\n",
        "\n",
        "filtered_data_9.loc[mask, 'score'] = filtered_data_9.loc[mask, 'e8']\n",
        "\n",
        "filtered_data_9= filtered_data_9.drop(columns=['e1','e2','e3','e4','e5','e6','e7','e8','e9','e10','e11','e12','e13','e14', 'reason'])\n",
        "\n",
        "final_filtered = pd.concat([final_filtered, filtered_data_9], ignore_index=True)\n",
        "remaining_data = remaining_data.drop(filtered_data_9.index)\n",
        "\n",
        "print('nineth iteration : ', len(final_filtered))\n",
        "print('total:', len(final_filtered)+len(remaining_data))\n",
        "\n",
        "#TENTH ITERATION\n",
        "filtered_data_10 = remaining_data[\n",
        "    remaining_data['e9'].notna() &\n",
        "    remaining_data[columns_to_check[9:]].isna().all(axis=1)\n",
        "]\n",
        "\n",
        "mask = (\n",
        "    remaining_data['e9'].notna() &\n",
        "    remaining_data[columns_to_check[9:]].isna().all(axis=1)\n",
        ")\n",
        "\n",
        "filtered_data_10.loc[mask, 'score'] = filtered_data_10.loc[mask, 'e9']\n",
        "\n",
        "filtered_data_10= filtered_data_10.drop(columns=['e1','e2','e3','e4','e5','e6','e7','e8','e9','e10','e11','e12','e13','e14', 'reason'])\n",
        "\n",
        "final_filtered = pd.concat([final_filtered, filtered_data_10], ignore_index=True)\n",
        "remaining_data = remaining_data.drop(filtered_data_10.index)\n",
        "\n",
        "print('tenth iteration : ', len(final_filtered))\n",
        "print('total:', len(final_filtered)+len(remaining_data))\n",
        "\n",
        "#ELEVENTH ITERATION\n",
        "filtered_data_11 = remaining_data[\n",
        "    remaining_data['e10'].notna() &\n",
        "    remaining_data[columns_to_check[10:]].isna().all(axis=1)\n",
        "]\n",
        "\n",
        "mask = (\n",
        "    remaining_data['e10'].notna() &\n",
        "    remaining_data[columns_to_check[10:]].isna().all(axis=1)\n",
        ")\n",
        "\n",
        "filtered_data_11.loc[mask, 'score'] = filtered_data_11.loc[mask, 'e10']\n",
        "\n",
        "filtered_data_11= filtered_data_11.drop(columns=['e1','e2','e3','e4','e5','e6','e7','e8','e9','e10','e11','e12','e13','e14', 'reason'])\n",
        "\n",
        "final_filtered = pd.concat([final_filtered, filtered_data_11], ignore_index=True)\n",
        "remaining_data = remaining_data.drop(filtered_data_11.index)\n",
        "\n",
        "print('eleven iteration : ', len(final_filtered))\n",
        "print('total:', len(final_filtered)+len(remaining_data))\n",
        "\n",
        "#TWELVETH ITERATION\n",
        "filtered_data_12 = remaining_data[\n",
        "    remaining_data['e11'].notna() &\n",
        "    remaining_data[columns_to_check[11:]].isna().all(axis=1)\n",
        "]\n",
        "\n",
        "mask = (\n",
        "    remaining_data['e11'].notna() &\n",
        "    remaining_data[columns_to_check[11:]].isna().all(axis=1)\n",
        ")\n",
        "\n",
        "filtered_data_12.loc[mask, 'score'] = filtered_data_12.loc[mask, 'e11']\n",
        "\n",
        "filtered_data_12= filtered_data_12.drop(columns=['e1','e2','e3','e4','e5','e6','e7','e8','e9','e10','e11','e12','e13','e14', 'reason'])\n",
        "\n",
        "final_filtered = pd.concat([final_filtered, filtered_data_12], ignore_index=True)\n",
        "remaining_data = remaining_data.drop(filtered_data_12.index)\n",
        "\n",
        "print('twelve iteration : ', len(final_filtered))\n",
        "print('total:', len(final_filtered)+len(remaining_data))\n",
        "\n",
        "\n",
        "#THIRTEENTH ITERATION\n",
        "filtered_data_13 = remaining_data[\n",
        "    remaining_data['e12'].notna() &\n",
        "    remaining_data[columns_to_check[12:]].isna().all(axis=1)\n",
        "]\n",
        "\n",
        "mask = (\n",
        "    remaining_data['e12'].notna() &\n",
        "    remaining_data[columns_to_check[12:]].isna().all(axis=1)\n",
        ")\n",
        "\n",
        "filtered_data_13.loc[mask, 'score'] = filtered_data_13.loc[mask, 'e12']\n",
        "\n",
        "filtered_data_13= filtered_data_13.drop(columns=['e1','e2','e3','e4','e5','e6','e7','e8','e9','e10','e11','e12','e13','e14', 'reason'])\n",
        "\n",
        "final_filtered = pd.concat([final_filtered, filtered_data_13], ignore_index=True)\n",
        "remaining_data = remaining_data.drop(filtered_data_13.index)\n",
        "\n",
        "print('thirteen iteration : ', len(final_filtered))\n",
        "print('total:', len(final_filtered)+len(remaining_data))\n",
        "\n",
        "#FOURTEEN ITERATION\n",
        "filtered_data_14 = remaining_data[\n",
        "    remaining_data['e13'].notna() &\n",
        "    remaining_data[columns_to_check[13:]].isna().all(axis=1)\n",
        "]\n",
        "\n",
        "mask = (\n",
        "    remaining_data['e13'].notna() &\n",
        "    remaining_data[columns_to_check[13:]].isna().all(axis=1)\n",
        ")\n",
        "\n",
        "filtered_data_14.loc[mask, 'score'] = filtered_data_14.loc[mask, 'e13']\n",
        "\n",
        "filtered_data_14= filtered_data_14.drop(columns=['e1','e2','e3','e4','e5','e6','e7','e8','e9','e10','e11','e12','e13','e14', 'reason'])\n",
        "\n",
        "final_filtered = pd.concat([final_filtered, filtered_data_14], ignore_index=True)\n",
        "remaining_data = remaining_data.drop(filtered_data_14.index)\n",
        "\n",
        "print('thirteen iteration : ', len(final_filtered))\n",
        "print('total:', len(final_filtered)+len(remaining_data))\n",
        "\n",
        "#FIFTEEN  ITERATION\n",
        "\n",
        "\n",
        "mask = (\n",
        "    remaining_data['e14'].notna()\n",
        ")\n",
        "\n",
        "remaining_data.loc[mask, 'score'] = remaining_data.loc[mask, 'e14']\n",
        "\n",
        "remaining_data= remaining_data.drop(columns=['e1','e2','e3','e4','e5','e6','e7','e8','e9','e10','e11','e12','e13','e14', 'reason'])\n",
        "\n",
        "final_filtered = pd.concat([final_filtered, remaining_data], ignore_index=True)\n",
        "remaining_data = remaining_data.drop(remaining_data.index)\n",
        "\n",
        "print('fifteen iteration : ', len(final_filtered))\n",
        "print('total:', len(final_filtered)+len(remaining_data))"
      ],
      "metadata": {
        "id": "0Zxw6sz9RR-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nan_rows = final_filtered[final_filtered['score'].isna()]\n",
        "print(len(nan_rows))\n",
        "print(nan_rows)"
      ],
      "metadata": {
        "id": "YNF4syZsRz0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_csv = '/Users/alicemicheli/Desktop/THESIS MASTER/Data_for_thesis/erased_columns_KC_extraction_remaining.csv'#USE THISSSSSSS\n",
        "final_filtered.to_csv(output_csv, index=False)"
      ],
      "metadata": {
        "id": "HilNhzmHUTzS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}