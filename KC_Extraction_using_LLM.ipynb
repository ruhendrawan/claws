{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrOLixoP4WP8"
      },
      "source": [
        "# Google Drive\n",
        "\n",
        "Prepare Connection to Google Drive to download Code Snippets and Ontology"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9a8mO7qKq3nW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZNdXO1zqrIe"
      },
      "source": [
        "## Ontology\n",
        "\n",
        "Generate list of leave nodes as CSV for LLM Prompt Context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dEHzB-UQRHl",
        "outputId": "c854fef0-5aec-4b9f-cf73-5df0e66baf59"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q rdflib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9H-SmF-HQJxH",
        "outputId": "2ef88c27-ca54-45ab-a899-d4b6ba4690d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV leaf outputs with parent and description saved.\n"
          ]
        }
      ],
      "source": [
        "from rdflib import Graph, RDFS, RDF\n",
        "import csv\n",
        "import json\n",
        "\n",
        "def clean_uri(uri):\n",
        "    return uri.split('#')[-1] if '#' in uri else uri.split('/')[-1]\n",
        "\n",
        "def build_class_hierarchy(rdf_file_path, format='xml'):\n",
        "    g = Graph()\n",
        "    g.parse(rdf_file_path, format=format)\n",
        "\n",
        "    hierarchy = {}\n",
        "    class_labels = set()\n",
        "    comments = {}\n",
        "\n",
        "    for s, p, o in g:\n",
        "        if p == RDFS.subClassOf:\n",
        "            child = clean_uri(str(s))\n",
        "            parent = clean_uri(str(o))\n",
        "\n",
        "            class_labels.add(child)\n",
        "            class_labels.add(parent)\n",
        "\n",
        "            if parent not in hierarchy:\n",
        "                hierarchy[parent] = []\n",
        "            hierarchy[parent].append(child)\n",
        "\n",
        "        if p == RDFS.comment:\n",
        "            comments[clean_uri(str(s))] = str(o)\n",
        "\n",
        "    for label in class_labels:\n",
        "        hierarchy.setdefault(label, [])\n",
        "\n",
        "    return hierarchy, comments\n",
        "\n",
        "def save_dict_to_text(data, output_file_path):\n",
        "    with open(output_file_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(json.dumps(data, indent=2))\n",
        "\n",
        "def extract_subtree(hierarchy, root):\n",
        "    subtree = {}\n",
        "\n",
        "    def dfs(node):\n",
        "        if node not in hierarchy:\n",
        "            return\n",
        "        subtree[node] = hierarchy[node]\n",
        "        for child in hierarchy[node]:\n",
        "            dfs(child)\n",
        "\n",
        "    dfs(root)\n",
        "    return subtree\n",
        "\n",
        "def extract_leaves_with_parents(subtree):\n",
        "    leaves_with_parents = []\n",
        "    for parent, children in subtree.items():\n",
        "        for child in children:\n",
        "            if child in subtree and not subtree[child]:\n",
        "                leaves_with_parents.append((parent, child))\n",
        "    return leaves_with_parents\n",
        "\n",
        "code_patterns = {\n",
        "    \"IfCheckingInForLoop\": \"refers to an if statement inside a for loop\",\n",
        "    \"IfElifCheckingInForLoop\": \"refers to an if-elif statement inside a for loop\",\n",
        "    \"IfElifElseCheckingInForLoop\": \"refers to an if-elif-else statement inside a for loop\",\n",
        "    \"IfElseCheckingInForLoop\": \"refers to an if-else statement inside a for loop\",\n",
        "    \"IfCheckingInWhileLoop\": \"refers to an if statement inside a while loop\",\n",
        "    \"IfElifCheckingInWhileLoop\": \"refers to an if-elif statement inside a while loop\",\n",
        "    \"IfElifElseCheckingInWhileLoop\": \"refers to an if-elif-else statement inside a while loop\",\n",
        "    \"IfElseCheckingInWhileLoop\": \"refers to an if-else statement inside a while loop\",\n",
        "    \"IfChecking\": \"refers to an if statement which is not placed inside any loop\",\n",
        "    \"IfElifChecking\": \"refers to an if-elif statement which is not placed inside any loop\",\n",
        "    \"IfElifElseChecking\": \"refers to an if-elif-else statement which is not placed inside any loop\",\n",
        "    \"IfElseChecking\": \"refers to an if-else statement which is not placed inside any loop\",\n",
        "    \"NestedIfChecking\": \"refers to an if inside another if\",\n",
        "    \"ListReferencing\": \"refers to the code in which one list is set equal to another, and a change to one of the lists causes the same change in the other list. ListReferencing should only be marked as present if a list is explicitly assigned to another list variable (e.g., list2 = list1).\",\n",
        "    \"AccessingDictionary\": \"includes any operation that retrieves values from a dictionary, including direct indexing (dict[key]) and methods like .get(), .items(), .keys(), and .values().\",\n",
        "    \"MixedNestedLoopIteration\": \"refers to nested loop such that a while-loop inside a for-loop or a for-loop inside a while-loop\",\n",
        "    \"NestedForLoopIteration\": \"refers to nested loop such that a for-loop is inside another for-loop\",\n",
        "    \"NestedWhileLoopIteration\": \"refers to nested loop such that a while-loop is inside another while-loop\",\n",
        "    \"SingleForLoopIteration\": \"refers to a single use use of a for-loop that has no nested structure\",\n",
        "    \"SingleWhileLoopIteration\": \"refers to a single use of a while-loop that has no nested structure\",\n",
        "    \"CallingFunctionLibrary\": \"refers to ANY use of built-in Python functions (print, len, replace, etc.) and built-in methods of objects (like list.append() or string.replace(), etc.).\",\n",
        "    \"CallingNestedFunction\": \"refers to calling a function that was defined inside another user-defined function, but the call itself can happen anywhere (inside or outside the enclosing function).\",\n",
        "    \"DefiningNestedFunction\": \"refers to defining a function inside another user-defined function.\",\n",
        "    \"CallingRecursiveFunction\": \"refers to calling a function that calls itself (recursion)\",\n",
        "    \"DefiningRecursiveFunction\": \"refers to defining a function that calls itself within its own body (recursion).\",\n",
        "    \"NestedFunctionCall\": \"refers to when one function call is placed as an argument to another function call (e.g., f(g(x)))\",\n",
        "    \"DefiningStandardFunction\": \"refers to defining a function that: 1) Does not call any other user-defined functions 2) Is not nested inside another function 3) Does not call itself (not recursive) Note: A standard function may still call built-in functions. Additionally, DefiningStandardFunction can still be later called in nested patterns (NestedFunctionCall).\",\n",
        "    \"CallingStandardFunction\": \"refers to calling a function that meets all the 3 following criteria:1) Does not call any other user-defined functions 2) Is not nested inside another function 3) Does not call itself (not recursive) Note: A standard function may still call built-in functions.\"\n",
        "}\n",
        "\n",
        "\n",
        "leaf_filter = [\n",
        "    # Python Parser\n",
        "    'UnaryOperation',\n",
        "    'IndexingExpression',\n",
        "    'SlicingExpression',\n",
        "    # Educational\n",
        "    'IndexingDictionary',\n",
        "    'IndexingList',\n",
        "    'IndexingString',\n",
        "    'IndexingTuple',\n",
        "    'SlicingList',\n",
        "    'SlicingTuple',\n",
        "    'SlicingString',\n",
        "    'WhileLoopWithListIndexing',\n",
        "    'WhileLoopWith*=',\n",
        "    'WhileLoopWith+=',\n",
        "    'ForLoopWithListIndexing',\n",
        "    'ForLoopWith*=',\n",
        "    'ForLoopWith+=',\n",
        "    'ReplacingDictionaryElement',\n",
        "    'ReplacingListElement',\n",
        "    'ReplacingElement2DArray'\n",
        "]\n",
        "leaf_filter = None\n",
        "\n",
        "def save_leaves_with_parents_csv(leaves_with_parents, output_csv_path, comments):\n",
        "    with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['Parent', 'Leaf', 'Description'])\n",
        "        for parent, leaf in leaves_with_parents:\n",
        "\n",
        "            if parent == \"EducationalPython\":\n",
        "                parent = \"Python\"\n",
        "            if parent == \"Iteration\":\n",
        "                parent = \"NonNestedIteration\"\n",
        "            if parent == \"ModifyingStrigCase\":\n",
        "                parent = \"ModifyingStringCase\"\n",
        "            if parent == \"CallingStandardFunction\" and leaf == \"NestedCall\":\n",
        "                continue\n",
        "\n",
        "            is_leaf_included = True\n",
        "            if leaf_filter:\n",
        "              is_leaf_included = leaf in leaf_filter\n",
        "\n",
        "            if is_leaf_included:\n",
        "              description = comments.get(leaf) or code_patterns.get(leaf, '')\n",
        "              writer.writerow([parent, leaf, description])\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    rdf_file = \"data/final_ontology.owl\"\n",
        "    rdf_format = 'xml'\n",
        "\n",
        "    hierarchy, comments = build_class_hierarchy(rdf_file, format=rdf_format)\n",
        "\n",
        "    save_dict_to_text(hierarchy, \"ontology/hierarchy.txt\")\n",
        "\n",
        "    educational_python_subtree = extract_subtree(hierarchy, \"EducationalPython\")\n",
        "    python_subtree = extract_subtree(hierarchy, \"Python\")\n",
        "\n",
        "    save_dict_to_text(educational_python_subtree, \"ontology/hierarchy_educational_python.txt\")\n",
        "    save_dict_to_text(python_subtree, \"ontology/hierarchy_python.txt\")\n",
        "\n",
        "    educational_python_leaves = extract_leaves_with_parents(educational_python_subtree)\n",
        "    python_leaves = extract_leaves_with_parents(python_subtree)\n",
        "\n",
        "    save_leaves_with_parents_csv(educational_python_leaves, \"ontology/hierarchy_educational_python_leaves.csv\", comments)\n",
        "    save_leaves_with_parents_csv(python_leaves, \"ontology/hierarchy_python_leaves.csv\", comments)\n",
        "\n",
        "    print(\"CSV leaf outputs with parent and description saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJtXbdE_qvQm"
      },
      "source": [
        "## Code Snippets\n",
        "\n",
        "Extract every code snippet. Store the snippets in a list. In the next phase, feed each snippet to the LLM to pull out its knowledge components (KCs).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adYMFANx8RyE"
      },
      "source": [
        "Parsing Snippets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RyZja2-xrUbM"
      },
      "outputs": [],
      "source": [
        "def split_code_exercises(input_file):\n",
        "  with open(input_file, 'r') as f:\n",
        "    code_exercises = f.read()\n",
        "  code_snippets = code_exercises.split('#CODE_SNIPPET')\n",
        "  for idx, snippet in enumerate(code_snippets):\n",
        "    snippet = snippet.strip()\n",
        "  code_snippets = [s.strip() for s in code_snippets if s.strip()]\n",
        "  return code_snippets\n",
        "\n",
        "def test_split_code_exercises():\n",
        "  # input_file = \"data/all_exercise_snippet.py\"\n",
        "  input_file = \"data/all_exercise_snippet_with_prob_description.py\"\n",
        "  code_snippets = split_code_exercises(input_file)\n",
        "  print(len(code_snippets))\n",
        "  print(code_snippets[0])\n",
        "\n",
        "# test_split_code_exercises()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCicB4lmCr5F"
      },
      "source": [
        "## Batch Generation Classification with Window Sliding (CLAWS)\n",
        "\n",
        "Main settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_R6FP2kuC5F_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import csv\n",
        "import json\n",
        "\n",
        "\n",
        "\n",
        "# SNIPPET_TYPE = \"code_only\"\n",
        "# PROMPT_INPUT_SNIPPET = \"all_exercise_snippet.py\"\n",
        "\n",
        "# SNIPPET_TYPE = \"data/code_with_problems\"\n",
        "# PROMPT_INPUT_SNIPPET = \"data/all_exercise_snippet_with_prob_description.py\"\n",
        "\n",
        "\n",
        "# PROMPT_SESSION_ID = \"window_10\"\n",
        "# PROMPT_WINDOW_SIZE = 10\n",
        "\n",
        "# PROMPT_SESSION_ID = \"window_5\"\n",
        "# PROMPT_WINDOW_SIZE = 5\n",
        "\n",
        "# PROMPT_SESSION_ID = \"no_window\"\n",
        "# PROMPT_WINDOW_SIZE = -1\n",
        "\n",
        "\n",
        "\n",
        "SNIPPET_TYPE = \"data/parsons_with_problems\"\n",
        "PROMPT_INPUT_SNIPPET = \"data/parsons_with_prob_description.py\"\n",
        "\n",
        "#\n",
        "# PROMPT_SESSION_ID = \"parsons_window_10\"\n",
        "# PROMPT_WINDOW_SIZE = 10\n",
        "\n",
        "PROMPT_SESSION_ID = \"parsons_window_5\"\n",
        "PROMPT_WINDOW_SIZE = 5\n",
        "\n",
        "PROMPT_OUTPUT_PATH = f\"output/{PROMPT_SESSION_ID}/{SNIPPET_TYPE}/prompts\"\n",
        "CLAWS_OUTPUT_PATH = f\"output/{PROMPT_SESSION_ID}/{SNIPPET_TYPE}/output\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZGgZeSI5Fjk"
      },
      "source": [
        "### Generate Prompt from Templates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSx6WnOa5QkI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Window Size = 5\n",
            "Python Classes = 98\n",
            "Educational Chunks Num. of Windows = 20\n",
            "Educational Python Classes = 108\n",
            "Educational Python Num. of Windows = 22\n",
            "Generated 115 code snippets x 42 ontology batches\n",
            "Output path: output/parsons_window_5/data/parsons_with_problems/prompts\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "def get_code_exercises(input_file):\n",
        "  snippets = split_code_exercises(input_file)\n",
        "  return snippets\n",
        "\n",
        "\n",
        "\n",
        "def chunk_list(lst, chunk_size):\n",
        "  if chunk_size <= 0:\n",
        "    return [lst]\n",
        "  return [lst[i:i+chunk_size] for i in range(0, len(lst), chunk_size)]\n",
        "\n",
        "def get_classes(window_size):\n",
        "  print(f\"Window Size = {window_size}\")\n",
        "\n",
        "  t = open(\"ontology/hierarchy_python_leaves.csv\", \"r\").readlines()\n",
        "  t = t[1:]\n",
        "  print(f\"Python Classes = {len(t)}\")\n",
        "  python_csv_lines = chunk_list(t, window_size)\n",
        "  print(f\"Educational Chunks Num. of Windows = {len(python_csv_lines)}\")\n",
        "\n",
        "  t = open(\"ontology/hierarchy_educational_python_leaves.csv\", \"r\").readlines()\n",
        "  t = t[1:]\n",
        "  print(f\"Educational Python Classes = {len(t)}\")\n",
        "  educational_python_csv_lines = chunk_list(t, window_size)\n",
        "  print(f\"Educational Python Num. of Windows = {len(educational_python_csv_lines)}\")\n",
        "\n",
        "  csv_lines = python_csv_lines + educational_python_csv_lines\n",
        "  exit()\n",
        "  return csv_lines\n",
        "\n",
        "\n",
        "\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "You are an annotation assistant.\n",
        "Given a code snippet, evaluate all rows in the ontology classes CSV.\n",
        "Provide a reason for how the class is being used in the snippet/ problem, evaluate, and give a usage score.\n",
        "\n",
        "## Output\n",
        "Respond as long as possible.\n",
        "Score is between 0-5.\n",
        "Return as a CSV (exercise_name,parent,leaf,reason,score).\n",
        "\n",
        "## INPUT: Ontology Classes\n",
        "```csv(parent,leaf,description)\n",
        "{ontology}\n",
        "```\n",
        "\n",
        "## INPUT: Code Snippet\n",
        "```code\n",
        "{snippet}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "def generate_prompts(snippet_file, window_size, output_dir):\n",
        "    shutil.rmtree(output_dir, ignore_errors=True)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    snippets = split_code_exercises(snippet_file)\n",
        "    class_windows = get_classes(window_size)\n",
        "    for code_idx, snippet in enumerate(snippets, start=1):\n",
        "        for class_idx, csv_data in enumerate(class_windows, start=1):\n",
        "            ontology = \"\\n\".join([line.strip() for line in csv_data])\n",
        "            prompt = PROMPT_TEMPLATE.format(ontology=ontology, snippet=snippet)\n",
        "\n",
        "            # print(prompt)\n",
        "            # break\n",
        "\n",
        "            fname = f\"{class_idx:03d}-{code_idx:03d}.txt\"\n",
        "            with open(Path(output_dir) / fname, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(prompt)\n",
        "    print(f\"Generated {len(snippets)} code snippets x {len(class_windows)} ontology batches\")\n",
        "    print(f\"Output path: {output_dir}\")\n",
        "\n",
        "\n",
        "\n",
        "generate_prompts(\n",
        "    PROMPT_INPUT_SNIPPET,\n",
        "    PROMPT_WINDOW_SIZE,\n",
        "    PROMPT_OUTPUT_PATH\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-4uxbVyQmsq"
      },
      "source": [
        "### Prepare Batch Completions (OpenAI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "BPJtOOr6lVtp",
        "outputId": "09f63907-36ea-4679-f220-b02aeb1d0210"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JSONL file created: output/parsons_window_5/data/parsons_with_problems/prompts/class_bulk.jsonl\n"
          ]
        }
      ],
      "source": [
        "def create_jsonl_from_prompts(prompt_dir, jsonl_out):\n",
        "    entries = []\n",
        "    for prompt_file in sorted(Path(prompt_dir).glob(\"*.txt\")):\n",
        "        class_idx, code_idx = prompt_file.stem.split(\"-\")\n",
        "        with open(prompt_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            prompt = f.read()\n",
        "        entries.append({\n",
        "            \"custom_id\": f\"{class_idx}-{code_idx}\",\n",
        "            \"method\": \"POST\",\n",
        "            \"url\": \"/v1/chat/completions\",\n",
        "            \"body\": {\n",
        "                \"model\": \"gpt-4.1-mini\",\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"system\", \"content\": prompt}\n",
        "                ],\n",
        "                \"max_tokens\": 32768,\n",
        "                \"temperature\": 0,\n",
        "                \"top_p\": 0.1,\n",
        "                \"store\": True\n",
        "            }\n",
        "        })\n",
        "    with open(jsonl_out, \"w\", encoding=\"utf-8\") as f:\n",
        "        for item in entries:\n",
        "            f.write(json.dumps(item) + \"\\n\")\n",
        "    print(f\"JSONL file created: {jsonl_out}\")\n",
        "\n",
        "\n",
        "\n",
        "create_jsonl_from_prompts(\n",
        "  PROMPT_OUTPUT_PATH,\n",
        "  Path(PROMPT_OUTPUT_PATH) / \"class_bulk.jsonl\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0y7cpywE7Mez",
        "outputId": "4d3cf1e2-2b99-43fd-b142-7b9de052267c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File 'output/parsons_window_5/data/parsons_with_problems/prompts/class_bulk.jsonl' has 4831 lines.\n",
            "{\"custom_id\": \"001-001\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-4.1\n"
          ]
        }
      ],
      "source": [
        "def preview_file(filename, num_chars=100):\n",
        "    \"\"\"\n",
        "    Prints the first num_chars characters of a file.\n",
        "\n",
        "    Args:\n",
        "        filename (str): The path to the file.\n",
        "        num_chars (int): The number of characters to preview (default is 100).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(filename, 'r') as file:\n",
        "            content = file.read()\n",
        "            lines_count = len(content.split('\\n'))\n",
        "            print(f\"File '{filename}' has {lines_count} lines.\")\n",
        "            print(content[:num_chars])\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{filename}' not found.\")\n",
        "    except Exception as e:\n",
        "         print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "preview_file(Path(PROMPT_OUTPUT_PATH) / \"class_bulk.jsonl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPbevuCpQubi"
      },
      "source": [
        "### --- Test Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sX1QI-qEQqcF"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q openai python-dotenv\n",
        "\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
        "\n",
        "def execute_prompt(prompt_file):\n",
        "  with open(prompt_file, \"r\") as f:\n",
        "    PROMPT = f.read()\n",
        "\n",
        "  # print(PROMPT)\n",
        "  # exit()\n",
        "\n",
        "  response = client.responses.create(\n",
        "    model=\"gpt-4.1-mini\",\n",
        "    input=[\n",
        "      {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": [\n",
        "          {\n",
        "            \"type\": \"input_text\",\n",
        "            \"text\": PROMPT\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "    ],\n",
        "    text={\n",
        "      \"format\": {\n",
        "        \"type\": \"text\"\n",
        "      }\n",
        "    },\n",
        "    reasoning={},\n",
        "    tools=[],\n",
        "    temperature=0,\n",
        "    max_output_tokens=32768,\n",
        "    top_p=0.1,\n",
        "    store=True\n",
        "  )\n",
        "  return response\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7PbHSRzDuW3",
        "outputId": "858f9912-8ea0-46bc-e9a1-20862a663de8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "exercise_name,parent,leaf,reason,score\n",
            "ps_hello,BuiltInFunction,min(),The code snippet does not use the min() function or any functionality related to it.,0\n",
            "ps_hello,BuiltInFunction,tuple(),The code snippet does not use the tuple() function or any tuple-related operations.,0\n",
            "ps_hello,BuiltInFunction,open(),The code snippet does not use the open() function or any file operations.,0\n",
            "ps_hello,BuiltInFunction,input(),The code snippet does not use the input() function or any user input operations.,0\n",
            "ps_hello,BuiltInFunction,next(),The code snippet does not use the next() function or any iterator operations.,0\n"
          ]
        }
      ],
      "source": [
        "def test_execute_prompt():\n",
        "  response = execute_prompt(Path(PROMPT_OUTPUT_PATH) / \"001-001.txt\")\n",
        "  content = response.output_text\n",
        "  print(content)\n",
        "\n",
        "test_execute_prompt()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRtmoVNnRCdz"
      },
      "source": [
        "### Upload and Create new Batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9iV6ItLy5sna"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6u4lLqnDuoq",
        "outputId": "c03392b9-8b02-4bc2-c2d1-c743d19c67c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Batch(id='batch_68c803f0da9481909445db299ebad875', completion_window='24h', created_at=1757938672, endpoint='/v1/chat/completions', input_file_id='file-WeFowKvSESs5tLeM4xGLi3', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1758025072, failed_at=None, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0), usage={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_tokens_details': {'cached_tokens': 0}, 'output_tokens_details': {'reasoning_tokens': 0}})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def upload_jsonl(openai_client, file_path):\n",
        "    return openai_client.files.create(file=open(file_path, \"rb\"), purpose=\"batch\")\n",
        "\n",
        "def create_batch(openai_client, input_file_id):\n",
        "    return openai_client.batches.create(\n",
        "        input_file_id=input_file_id,\n",
        "        endpoint=\"/v1/chat/completions\",\n",
        "        completion_window=\"24h\"\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "file_obj = upload_jsonl(\n",
        "  client,\n",
        "  Path(PROMPT_OUTPUT_PATH) / \"class_bulk.jsonl\"\n",
        ")\n",
        "# old_batch = batch\n",
        "batch = create_batch(client, file_obj.id)\n",
        "batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdzXcuibXlFW"
      },
      "source": [
        "### View Batch Detail or Cancel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-YxbMIOqPgP",
        "outputId": "734b7371-6fa2-49be-8af9-8e8a76968efa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Batch(id='batch_68c803f0da9481909445db299ebad875', completion_window='24h', created_at=1757938672, endpoint='/v1/chat/completions', input_file_id='file-WeFowKvSESs5tLeM4xGLi3', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1757942888, error_file_id=None, errors=None, expired_at=None, expires_at=1758025072, failed_at=None, finalizing_at=1757941219, in_progress_at=1757938737, metadata=None, output_file_id='file-SswGHckrd7f5SuSL316s8V', request_counts=BatchRequestCounts(completed=4830, failed=0, total=4830), usage={'input_tokens': 1338037, 'output_tokens': 904676, 'total_tokens': 2242713, 'input_tokens_details': {'cached_tokens': 0}, 'output_tokens_details': {'reasoning_tokens': 0}})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from openai.types import Batch, BatchRequestCounts\n",
        "\n",
        "def view_batch(openai_client, batch_id):\n",
        "    return openai_client.batches.retrieve(batch_id)\n",
        "\n",
        "def cancel_batch(openai_client, batch_id):\n",
        "    return openai_client.batches.cancel(batch_id)\n",
        "\n",
        "\n",
        "\n",
        "# # no_window\n",
        "# # code_only\n",
        "# batch_no_co = type('', (), {'id': \"batch_682475a840c88190ab9550734eb1cbfb\"})()\n",
        "# # code_with_problems\n",
        "# batch_no_cwp = Batch(id='batch_682496e7b9908190bff5a16f0a6e2632', completion_window='24h', created_at=1747228391, endpoint='/v1/chat/completions', input_file_id='file-Q3Arh24bcJ1wXdXzePboqu', object='batch', status='in_progress', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1747314791, failed_at=None, finalizing_at=None, in_progress_at=1747228393, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=528))\n",
        "\n",
        "# # window_5\n",
        "# # code_only\n",
        "# batch_5_co = Batch(id='batch_682499cb18f881908cec309fb7bff025', completion_window='24h', created_at=1747229131, endpoint='/v1/chat/completions', input_file_id='file-CzxwsBTMTWXh7cFist2GBD', object='batch', status='in_progress', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1747315531, failed_at=None, finalizing_at=None, in_progress_at=1747229138, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=11004))\n",
        "# # code_with_problems\n",
        "# batch_5_cwp = Batch(id='batch_68249b22fb488190bf5e19575a541c36', completion_window='24h', created_at=1747229474, endpoint='/v1/chat/completions', input_file_id='file-6UHuPFyN2usGHqGNDfd5Gi', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1747315874, failed_at=None, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
        "\n",
        "batch_view = view_batch(client, batch.id)\n",
        "# cancel_batch(client, batch.id)\n",
        "\n",
        "batch_view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZRK4aLZXovv"
      },
      "source": [
        "### Download Results and Save as Combined (CSV)\n",
        "\n",
        "TODO: Mark non-compliance to retry the generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zt4f5efJqLLG",
        "outputId": "a8500d9b-c0f3-42fd-a6c3-a1b33380dc8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded to: output/parsons_window_5/data/parsons_with_problems/output/batch_output.jsonl\n",
            "Combined CSV saved to: output/parsons_window_5/data/parsons_with_problems/output/result.csv\n"
          ]
        }
      ],
      "source": [
        "def download_batch_result(openai_client, output_file_id, output_path):\n",
        "    content = openai_client.files.content(output_file_id)\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(content.text)\n",
        "    print(f\"Downloaded to: {output_path}\")\n",
        "\n",
        "\n",
        "\n",
        "def combine_results_to_csv(jsonl_file, csv_out):\n",
        "    rows = []\n",
        "    with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        count = 0\n",
        "        for line in f:\n",
        "            count += 1\n",
        "            data = json.loads(line)\n",
        "            custom_id = data.get(\"custom_id\")\n",
        "            content = data.get(\"response\", {}).get(\"body\", {}).get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
        "            try:\n",
        "                for row in csv.reader(content.splitlines()):\n",
        "                    if row and row[0] != \"exercise_name\":\n",
        "                        rows.append(row)\n",
        "            except csv.Error as e:\n",
        "                print(f\"CSV parsing error in entry {count}: {e}\")\n",
        "                continue\n",
        "    with open(csv_out, \"w\", encoding=\"utf-8\", newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"exercise_name\", \"parent\", \"leaf_class\", \"reason\", \"score\"])\n",
        "        writer.writerows(rows)\n",
        "    print(f\"Combined CSV saved to: {csv_out}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "os.makedirs(CLAWS_OUTPUT_PATH, exist_ok=True)\n",
        "download_batch_result(\n",
        "  client,\n",
        "  batch_view.output_file_id,\n",
        "  Path(CLAWS_OUTPUT_PATH) / \"batch_output.jsonl\"\n",
        ")\n",
        "combine_results_to_csv(\n",
        "  Path(CLAWS_OUTPUT_PATH) / \"batch_output.jsonl\",\n",
        "  Path(CLAWS_OUTPUT_PATH) / \"result.csv\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "EPbevuCpQubi"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
